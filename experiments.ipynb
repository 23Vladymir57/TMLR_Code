{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import dataloader as dl\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import *\n",
    "from models import *\n",
    "from DFA2SRN import machine_proces, dfa2srn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = np.log(2)*128\n",
    "T_U  = J*torch.tensor([[2,0],[0,2],[2,0],[0,2]])\n",
    "T_Ub =  torch.tensor([0,0,0,0])\n",
    "T_W  = -J*torch.tensor([[3,3,1,1],[1,1,3,3],[1,1,3,3],[3,3,1,1]])\n",
    "T_Wb =  torch.tensor([0,0,0,0])\n",
    "T_V  = J*torch.tensor([-1,-1,1,1]) \n",
    "T_Vb = torch.tensor([0])\n",
    "\n",
    "target = [T_U,T_Ub,T_W,T_Wb,T_V,T_Vb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the training data for: \"the language of all words with an odd number of a's\" on the alphabet {a,b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "training_set = []\n",
    "for k in range(1,50):\n",
    "    if k==1:\n",
    "        training_set += random_strings(k,2)\n",
    "    elif 2<=k<=5:\n",
    "        training_set += random_strings(k,4)\n",
    "    else:\n",
    "        training_set += random_strings(k,10)\n",
    "print(len(training_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg_pl_loss     = []\n",
    "reg_pl_norm_2   = []\n",
    "reg_pl_norm_inf = []\n",
    "reg_pl_dist     = []\n",
    "reg_pl_para     = []\n",
    "q_h = Quasy_H()\n",
    "net = SRN(4,2,1,num_layers=1, activation= 'sig').to(dtype = torch.float32)\n",
    "B = False\n",
    "A = True\n",
    "j=0\n",
    "for j in range(20000): ## in the article the number of batches is 20000\n",
    "    if j%100 == 0:\n",
    "        print(\"Epoch :\"+str(j))\n",
    "    (plot_loss,plot_norm_2,plot_norm_inf,plot_dist,plot_para,A) = gD_regular(net,target,q_h ,training_set,0.001,60,B)\n",
    "    reg_pl_loss += plot_loss\n",
    "    reg_pl_norm_2 += plot_norm_2\n",
    "    reg_pl_norm_inf += plot_norm_inf\n",
    "    reg_pl_dist += plot_dist\n",
    "    reg_pl_para += plot_para"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4,figsize=(28,6))\n",
    "\n",
    "fig.tight_layout(w_pad=2)\n",
    "\n",
    "K = 26\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "ax1.plot(reg_pl_loss, color='#007291')\n",
    "ax1.plot(rolling_avg(reg_pl_loss,3),color='#d8460b')\n",
    "ax1.set_title(\"Loss\", fontsize = K)\n",
    "\n",
    "ax2.plot(reg_pl_norm_2, color='#007291')\n",
    "ax2.plot(rolling_avg(reg_pl_norm_2,3),color='#d8460b')\n",
    "ax2.set_title(\"$\\|\\\\mathbf{p}_{k} - \\\\mathbf{p}_{k-1}\\|$\", fontsize = K)\n",
    "\n",
    "ax3.plot(reg_pl_norm_inf, color='#007291')\n",
    "ax3.plot(rolling_avg(reg_pl_norm_inf,3),color='#d8460b')\n",
    "ax3.set_title(\"$ \\|\\\\nabla \\mathcal{L}(\\mathcal{R}_{\\\\mathbf{p}_k})\\|_\\infty $\", fontsize = K)\n",
    "\n",
    "ax4.plot(reg_pl_dist,color='#007291')\n",
    "ax4.set_title(\"$\\|\\\\mathbf{p}_k - \\\\mathbf{p}_{Target}\\|$\", fontsize = K)\n",
    "\n",
    "plt.savefig('Regular_SGD_these.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD experiment on ML_Reg_Test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "names_path = \"names.txt\"\n",
    "path = \"data/Small/\"\n",
    "\n",
    "names, ext = lang_names(names_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = {}\n",
    "for j in range(len(names)):\n",
    "    name = names[j]\n",
    "    print(\"In processe \"+ name)\n",
    "    train_loader, val_loader, test_loader, input_length, seq_leng = data_creator2(name, path, ext)\n",
    "    print('The data is done')\n",
    "\n",
    "    ### defining the target parameters \n",
    "    machine_path = \"data/machines/\"\n",
    "    machine_name = name[:-1]+'.att'\n",
    "    T_m, D_m, num_state, alphabet_size = machine_proces(machine_path, machine_name)\n",
    "    target = dfa2srn(T_m, D_m)\n",
    "    \n",
    "\n",
    "    print('Starting the training')\n",
    "    model = customSRN(hidden_dim = num_state*alphabet_size ,input_dim=input_length, output_dim=1,seq_length=seq_leng,device=device,dtyp=32)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), # or any optimizer you prefer \n",
    "                            lr= 0.01, # 0.001 is used if no lr is specified\n",
    "                            momentum= 0.87)\n",
    "    ls_names = ['plot_loss','plot_norm','plot_norm2','plot_dist']\n",
    "    stats_data = STATS(ls_names)\n",
    "    train_stats(model,target, stats_data, optimizer, 32, train_loader, val_loader, 20000, 10**(-5), 0.01, dtype, device)\n",
    "    dc[name] = stats_data.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the obtained data from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4,figsize=(16.18,3.3))\n",
    "fig.tight_layout(w_pad=2)\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "for j in range(1):\n",
    "    name = names[j]\n",
    "    (plot_loss,plot_norm,plot_norm2,plot_dist) = dc[name]\n",
    "    l_min, l_max = np.min(plot_loss), np.max(plot_loss)\n",
    "    i_min, i_max = np.min(plot_norm), np.max(plot_norm)\n",
    "    n_min, n_max = np.min(plot_norm2), np.max(plot_norm2)\n",
    "\n",
    "    ## evolution of the Loss\n",
    "    ax[j][0].set_yscale(\"log\")\n",
    "    ax[j][0].plot(plot_loss, color='#007291')\n",
    "    ax[j][0].set_ylabel(names[j][:-1],size=18)\n",
    "    ax[j][0].plot(rolling_avg(plot_loss,3),color='#d8460b')\n",
    "    ax[j][0].set_title(\"Loss\")\n",
    "\n",
    "    ## Distance between two consecutive parameter vectors\n",
    "    ax[j][1].set_yscale(\"log\")\n",
    "    ax[j][1].plot(plot_norm2, color='#007291')\n",
    "    ax[j][1].plot(rolling_avg(plot_norm2,3),color='#d8460b')\n",
    "    ax[j][1].set_title(\"$\\|\\mathbf{p}_{k+1}- \\mathbf{p}_k\\|_2$\")\n",
    "\n",
    "    ## Infinite norme of the gradient\n",
    "    ax[j][2].set_yscale(\"log\")\n",
    "    ax[j][2].plot(plot_norm, color='#007291')\n",
    "    ax[j][2].plot(rolling_avg(plot_norm,3),color='#d8460b')\n",
    "    ax[j][2].set_title(\"$\\|\\\\nabla\\mathcal{L}(\\mathcal{R}_{\\mathbf{p}_k})\\|_\\infty$\")\n",
    "\n",
    "    ## distance to the target\n",
    "    ax[j][3].plot(plot_dist, color='#007291')\n",
    "    ax[j][3].set_title(\"$\\|\\mathbf{p}_{k}- \\mathbf{p}_{Target}\\|_2$\")\n",
    "    ax[j][0].set_ylabel(name)\n",
    "# plt.savefig('SGD_9_FL.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD experiment on ML_Reg_Test data set the training statistics that we have obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_20k.pkl\", 'rb') as picklefile:\n",
    "    data_dict = pickle.load(picklefile)\n",
    "fig, ax = plt.subplots(9,4,figsize=(16.18,30))\n",
    "fig.tight_layout(w_pad=2)\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "for j in range(9):\n",
    "    name = names[j]\n",
    "    (plot_loss,plot_norm,plot_norm2,plot_dist) = data_dict[name]\n",
    "    l_min, l_max = np.min(plot_loss), np.max(plot_loss)\n",
    "    i_min, i_max = np.min(plot_norm), np.max(plot_norm)\n",
    "    n_min, n_max = np.min(plot_norm2), np.max(plot_norm2)\n",
    "\n",
    "    ## evolution of the Loss\n",
    "    ax[j][0].set_yscale(\"log\")\n",
    "    ax[j][0].plot(plot_loss, color='#007291')\n",
    "    ax[j][0].set_ylabel(names[j][:-1],size=18)\n",
    "    ax[j][0].plot(rolling_avg(plot_loss,3),color='#d8460b')\n",
    "    ax[j][0].set_title(\"Loss\")\n",
    "\n",
    "    ## Distance between two consecutive parameter vectors\n",
    "    ax[j][1].set_yscale(\"log\")\n",
    "    ax[j][1].plot(plot_norm2, color='#007291')\n",
    "    ax[j][1].plot(rolling_avg(plot_norm2,3),color='#d8460b')\n",
    "    ax[j][1].set_title(\"$\\|\\mathbf{p}_{k+1}- \\mathbf{p}_k\\|_2$\")\n",
    "\n",
    "    ## Infinite norme of the gradient\n",
    "    ax[j][2].set_yscale(\"log\")\n",
    "    ax[j][2].plot(plot_norm, color='#007291')\n",
    "    ax[j][2].plot(rolling_avg(plot_norm,3),color='#d8460b')\n",
    "    ax[j][2].set_title(\"$\\|\\\\nabla\\mathcal{L}(\\mathcal{R}_{\\mathbf{p}_k})\\|_\\infty$\")\n",
    "\n",
    "    ## distance to the target\n",
    "    ax[j][3].plot(plot_dist, color='#007291')\n",
    "    ax[j][3].set_title(\"$\\|\\mathbf{p}_{k}- \\mathbf{p}_{Target}\\|_2$\")\n",
    "    ax[j][0].set_ylabel(name)\n",
    "# plt.savefig('SGD_9_FL.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "### main experiment ###\n",
    "#######################\n",
    "\n",
    "def gD(net,training_set,target,initial,eps,N):\n",
    "    plot_loss  = []\n",
    "    plot_norm  = []\n",
    "    plot_norm0 = []\n",
    "    plot_dist  = []\n",
    "    plot_grmin = []\n",
    "    plot_grmax = []\n",
    "    s = 0\n",
    "    t = 0\n",
    "    mean_loss = []\n",
    "    A = True\n",
    "    uu_max = 0\n",
    "    for  (word, label) in training_set:\n",
    "        \n",
    "        label = torch.tensor([label],dtype=torch.float32)\n",
    "        net.reset_h()\n",
    "        for lettre in word:\n",
    "            lettre = lettre.to(torch.float32)\n",
    "            outputs = net(lettre).to(torch.float32)\n",
    "        loss =  (-1)*( label*torch.log(outputs+10**(-7)) + (1-label)*torch.log((1-outputs)+10**(-7)) ) + torch.log(torch.tensor([1+10**(-7)]))  #criterion(outputs, label)\n",
    "        mean_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        (norm,norm_zero,u_min,u_max,norm_st) = fc_step(net,target,initial,(1/N),False)\n",
    "        reset_grad(net)\n",
    "        s+=1\n",
    "        if u_max < eps:\n",
    "            t +=1\n",
    "        if u_max > uu_max:\n",
    "            uu_max = u_max\n",
    "        uu_max = max(uu_max,u_max)\n",
    "        plot_norm.append(norm)\n",
    "        plot_norm0.append(norm_zero)\n",
    "        plot_dist.append(norm_st)\n",
    "        plot_grmin.append(u_min)\n",
    "        plot_loss.append(loss.item())\n",
    "        plot_grmax.append(u_max)\n",
    "        \n",
    "        A = not(t==len(training_set))\n",
    "    return (plot_loss,plot_norm,plot_norm0,plot_dist,plot_grmin,plot_grmax,A)\n",
    "\n",
    "def gD_persute(net,training_set,target,initial,eps,Lis):\n",
    "    N = Lis[0]\n",
    "    state = 0\n",
    "    plot_loss  = []\n",
    "    plot_norm  = []\n",
    "    plot_norm0 = []\n",
    "    plot_dist  = []\n",
    "    plot_grmin = []\n",
    "    plot_grmax = []\n",
    "    A= True \n",
    "    while A:\n",
    "        plot_loss_s,plot_norm_s,plot_norm0_s,plot_dist_s,plot_grmin_s,plot_grmax_s,A = gD(net,training_set,target,initial,eps,N)\n",
    "        \n",
    "        if A:\n",
    "            print('A is true')\n",
    "            plot_loss  += plot_loss_s\n",
    "            plot_norm  += plot_norm_s\n",
    "            plot_norm0 += plot_norm0_s\n",
    "            plot_dist  += plot_dist_s\n",
    "            plot_grmin += plot_grmin_s\n",
    "            plot_grmax += plot_grmax_s\n",
    "            fc_step(net,target,initial,(1/N),True)\n",
    "        elif not(A) and N != Lis[-1]:\n",
    "            print('A is not true')\n",
    "            print(str(N))\n",
    "            cf_step(net,target,initial,(1/N),True)\n",
    "            state += 1\n",
    "            N = Lis[state]\n",
    "            A=True\n",
    "        elif not(A) and N==Lis[-1]:\n",
    "            print('We reached the linit of the gradient')\n",
    "            dict = {}\n",
    "            dict['loss'] = plot_loss\n",
    "            dict['norm'] = plot_norm\n",
    "            dict['norm0']= plot_norm0\n",
    "            dict['dist'] = plot_dist\n",
    "            dict['grmin']= plot_grmin\n",
    "            dict['grmax']= plot_grmax\n",
    "            return dict\n",
    "            \n",
    "\n",
    "net = SRN(4,2,1,num_layers=1, activation= 'sig').to(dtype = torch.float32)\n",
    "initial = []\n",
    "for params in net.parameters():\n",
    "    initial.append(params.detach())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "eps = 10**(-14)\n",
    "\n",
    "# initializing the target parameters\n",
    "J = np.log(2)*128\n",
    "T_U  = J*torch.tensor([[2,0],[0,2],[2,0],[0,2]])\n",
    "T_Ub =  torch.tensor([0,0,0,0])\n",
    "T_W  = -J*torch.tensor([[3,3,1,1],[1,1,3,3],[1,1,3,3],[3,3,1,1]])\n",
    "T_Wb =  torch.tensor([0,0,0,0])\n",
    "T_V  = J*torch.tensor([-1,-1,1,1]) \n",
    "T_Vb = torch.tensor([0])\n",
    "target = [T_U,T_Ub,T_W,T_Wb,T_V,T_Vb]\n",
    "\n",
    "\n",
    "dict = gD_persute(net,training_set,target,initial,eps,[10**1, 10**2, 10**3])\n",
    "plot_loss = dict['loss']\n",
    "plot_norm = dict['norm']\n",
    "plot_norm0= dict['norm0']\n",
    "plot_dist = dict['dist']\n",
    "plot_grmin= dict['grmin']\n",
    "plot_grmax= dict['grmax']\n",
    "\n",
    "\n",
    "        \n",
    "#########################\n",
    "### backup experiment ###\n",
    "#########################\n",
    "\n",
    "def gD_part(net,training_set,target,initial,eps,B,N):\n",
    "    plot_loss  = []\n",
    "    plot_norm  = []\n",
    "    plot_norm0 = []\n",
    "    plot_dist  = []\n",
    "    plot_grmin = []\n",
    "    plot_grmax = []\n",
    "    s = 0\n",
    "    t = 0\n",
    "    mean_loss = []\n",
    "    A = True\n",
    "    uu_max = 0\n",
    "    for  (word, label) in training_set:\n",
    "        \n",
    "        label = torch.tensor([label],dtype=torch.float32)\n",
    "        net.reset_h()\n",
    "        for lettre in word:\n",
    "            lettre = lettre.to(torch.float32)\n",
    "            outputs = net(lettre).to(torch.float32)\n",
    "        loss =  (-1)*( label*torch.log(outputs+10**(-7)) + (1-label)*torch.log((1-outputs)+10**(-7)) ) + torch.log(torch.tensor([1+10**(-7)]))  #criterion(outputs, label)\n",
    "        mean_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        (norm,norm_zero,u_min,u_max,norm_st) = fc_partial_step(net,target,initial,(1/N),4,B,False) \n",
    "        reset_grad(net)\n",
    "        s+=1\n",
    "        if u_max < eps:\n",
    "            t +=1\n",
    "\n",
    "        plot_norm.append(norm)\n",
    "        plot_norm0.append(norm_zero)\n",
    "        plot_dist.append(norm_st)\n",
    "        plot_grmin.append(u_min)\n",
    "        plot_loss.append(loss.item())\n",
    "        plot_grmax.append(u_max)\n",
    "        \n",
    "        A = not(t==len(training_set))\n",
    "    return (plot_loss,plot_norm,plot_norm0,plot_dist,plot_grmin,plot_grmax,A)\n",
    "\n",
    "def gD_persute_part(net,training_set,target,initial,eps,B,Lis):\n",
    "    N = Lis[0]\n",
    "    state = 0\n",
    "    plot_loss  = []\n",
    "    plot_norm  = []\n",
    "    plot_norm0 = []\n",
    "    plot_dist  = []\n",
    "    plot_grmin = []\n",
    "    plot_grmax = []\n",
    "    A= True \n",
    "    count = 0\n",
    "    while A:\n",
    "        plot_loss_s,plot_norm_s,plot_norm0_s,plot_dist_s,plot_grmin_s,plot_grmax_s,A = gD_part(net,training_set,target,initial,eps,B,N)\n",
    "        \n",
    "        if A:\n",
    "            print('A is true')\n",
    "            plot_loss  += plot_loss_s\n",
    "            plot_norm  += plot_norm_s\n",
    "            plot_norm0 += plot_norm0_s\n",
    "            plot_dist  += plot_dist_s\n",
    "            plot_grmin += plot_grmin_s\n",
    "            plot_grmax += plot_grmax_s\n",
    "            if count <B:\n",
    "                fc_partial_step(net,target,initial,(1/N),4,B,True)\n",
    "                count+=1\n",
    "            else:\n",
    "                fc_partial_step(net,target,initial,(1/N),4,0,True)\n",
    "        elif not(A) and N != Lis[-1]:\n",
    "            print('A is not true')\n",
    "            print(str(N))\n",
    "            cf_partial_step(net,target,initial,(1/N),4,True)\n",
    "            state += 1\n",
    "            N = Lis[state]\n",
    "            A=True\n",
    "        elif not(A) and N==Lis[-1]:\n",
    "            print('We reached the linit of the gradient')\n",
    "            dict = {}\n",
    "            dict['loss'] = plot_loss\n",
    "            dict['norm'] = plot_norm\n",
    "            dict['norm0']= plot_norm0\n",
    "            dict['dist'] = plot_dist\n",
    "            dict['grmin']= plot_grmin\n",
    "            dict['grmax']= plot_grmax\n",
    "            return dict\n",
    "\n",
    "b_net = SRN(4,2,1,num_layers=1, activation= 'sig').to(dtype = torch.float32)\n",
    "initial = []\n",
    "for params in b_net.parameters():\n",
    "    initial.append(params.detach())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "eps = 10**(-14)\n",
    "\n",
    "J = np.log(2)*128\n",
    "T_U  = J*torch.tensor([[2,0],[0,2],[2,0],[0,2]])\n",
    "T_Ub =  torch.tensor([0,0,0,0])\n",
    "T_W  = -J*torch.tensor([[3,3,1,1],[1,1,3,3],[1,1,3,3],[3,3,1,1]])\n",
    "T_Wb =  torch.tensor([0,0,0,0])\n",
    "T_V  = J*torch.tensor([-1,-1,1,1]) \n",
    "T_Vb = torch.tensor([0])\n",
    "target = [T_U,T_Ub,T_W,T_Wb,T_V,T_Vb]\n",
    "\n",
    "\n",
    "dict_part = gD_persute_part(b_net,training_set,target,initial,eps,1,[10**1, 10**2, 10**3])\n",
    "part_plot_loss = dict_part['loss']\n",
    "part_plot_norm = dict_part['norm']\n",
    "part_plot_norm0= dict_part['norm0']\n",
    "part_plot_dist = dict_part['dist']\n",
    "part_plot_grmin= dict_part['grmin']\n",
    "part_plot_grmax= dict_part['grmax']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the experiment statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4,figsize=(22,10))\n",
    "K = 26\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "fig.tight_layout(h_pad=4)\n",
    "### main experiment\n",
    "\n",
    "ax[0][0].set_yscale(\"log\")\n",
    "ax[0][0].plot(plot_loss, color='#007291')\n",
    "ax[0][0].set_title(\"Loss\", fontsize = K)\n",
    "\n",
    "# ax[0][1].set_yscale(\"log\")\n",
    "ax[0][1].set_ylim(500,9*10**2)\n",
    "ax[0][1].plot(plot_dist, color='#007291')\n",
    "ax[0][1].set_title(\"$\\|\\\\mathbf{p}_k - \\\\mathbf{p}_{Target}\\|$\", fontsize = K)\n",
    "\n",
    "ax[0][2].set_yscale(\"log\")\n",
    "ax[0][2].plot(plot_grmax, color='#007291')\n",
    "ax[0][2].set_title(\"$ \\|\\\\nabla \\mathcal{L}(\\mathcal{R}_{\\\\mathbf{p}_k})\\|_\\infty $\", fontsize = K)\n",
    "\n",
    "labels, sizes = affected_params(net)\n",
    "\n",
    "ax[0][3].pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90, colors=[\"#f5c600\", \"#9b4923\"])\n",
    "### backup experiment\n",
    "\n",
    "ax[1][0].set_yscale(\"log\")\n",
    "ax[1][0].set_ylim(10**(-2),2)\n",
    "ax[1][0].plot(part_plot_loss,color='#d8460b')\n",
    "ax[1][0].set_title(\"Loss\", fontsize = K)\n",
    "\n",
    "# ax[1][1].set_yscale(\"log\")\n",
    "ax[1][1].set_ylim(500,9*10**2)\n",
    "ax[1][1].plot(part_plot_dist,color='#d8460b')\n",
    "ax[1][1].set_title(\"$\\|\\\\mathbf{p}_k - \\\\mathbf{p}_{Target}\\|$\", fontsize = K)\n",
    "\n",
    "ax[1][2].set_yscale(\"log\")\n",
    "ax[1][2].plot(part_plot_grmax,color='#d8460b')\n",
    "ax[1][2].set_title(\"$ \\|\\\\nabla \\mathcal{L}(\\mathcal{R}_{\\\\mathbf{p}_k})\\|_\\infty $\", fontsize = K)\n",
    "\n",
    "labels, sizes = affected_params(b_net)\n",
    "\n",
    "ax[1][3].pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90, colors=[\"#f5c600\", \"#9b4923\"])\n",
    "\n",
    "\n",
    "# plt.savefig('limits_of_gd_these.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
